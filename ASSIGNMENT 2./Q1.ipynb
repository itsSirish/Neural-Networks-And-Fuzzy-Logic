{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NNFL Assgn 2 Q1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qTGDBKmIcgS",
        "outputId": "c12770b1-01b4-4d72-f55e-0f5580b3a4ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/NNFL data/Data_A2/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_xOKcShJBXq",
        "outputId": "99739d57-8e42-4175-acec-4d721345de4e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/NNFL data/Data_A2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls -l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpF-4hC5Jd9B",
        "outputId": "55b12ef3-6423-43da-e7dd-2de40a8262f0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 86\n",
            "-rw------- 1 root root 70617 Apr 22 07:54 data55.xlsx\n",
            "-rw------- 1 root root 17039 Apr 29 07:25 data5.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "e1q5xwpNJly9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "  val = 1/(1+ np.exp(-x))\n",
        "  return val"
      ],
      "metadata": {
        "id": "GcMRxeUUJodJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoidDerivative(x):\n",
        "  val =  x * (1 - x)\n",
        "  return val"
      ],
      "metadata": {
        "id": "D_7vkEjQJqq8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perceptron(X_train_data, Y_train_data, bias, W, alpha = 0.001, epochs = 20000):\n",
        "  \n",
        "  for i in range(epochs):\n",
        "\n",
        "    layer = np.dot(X_train_data, W)\n",
        "    input = layer + bias\n",
        "    output = sigmoid(input)\n",
        "\n",
        "    error = output - Y_train_data\n",
        "    derivative = sigmoidDerivative(output)\n",
        "    update = error*derivative\n",
        "    WNew = np.dot(X_train_data.T, update)\n",
        "    W = W - alpha*WNew\n",
        "    update_bias = update\n",
        "    bias = bias - alpha*update\n",
        "\n",
        "  return W, bias\n",
        "def pred_eval(X, W, bias):\n",
        "  layer = np.dot(X, W)\n",
        "  input = layer + bias[0]\n",
        "  output = sigmoid(input)\n",
        "\n",
        "  return output"
      ],
      "metadata": {
        "id": "9KsWvRFIJu5-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resultQ1(filename = 'data55.xlsx'):\n",
        "  dataset = pd.read_excel(filename, header = None)\n",
        "\n",
        "  row, col = dataset.shape\n",
        "  feats = col - 1 \n",
        "\n",
        "  # normalization\n",
        "  dataset.loc[:, dataset.columns != feats] = (dataset.loc[:, dataset.columns != feats]-dataset.loc[:, dataset.columns != feats].mean(axis=0))/dataset.loc[:, dataset.columns != feats].std(axis=0)\n",
        "  \n",
        "  # spliting dataset into train test and val\n",
        "  training_data, validation_data, testing_data = np.split(dataset.sample(frac=1),[int(0.7*len(dataset)), int(0.8*len(dataset))])\n",
        "\n",
        "  training_data = np.array(training_data)\n",
        "  validation_data = np.array(validation_data)\n",
        "  testing_data = np.array(testing_data)\n",
        "  training_data_X = training_data[:, :feats]\n",
        "  training_data_y = training_data[:, feats]\n",
        "  validation_data_X = validation_data[:, :feats]\n",
        "  validation_data_y = validation_data[:, feats]\n",
        "  testing_data_X = testing_data[:, :feats]\n",
        "  testing_data_y = testing_data[:, feats]\n",
        "\n",
        "  train_row, train_col = training_data_X.shape\n",
        "\n",
        "  W = np.random.randn(train_col)  \n",
        "  bias = np.ones(train_row)\n",
        "\n",
        "  W, bias = perceptron(training_data_X, training_data_y, bias, W)\n",
        "  print(\"The Weights after training is as follows: \\n\")\n",
        "  pprint(W)\n",
        "  print(\"The Bias after training is as follows: \", bias[0])\n",
        "\n",
        "  train_pred = pred_eval(training_data_X, W, bias)\n",
        "  train_pred = np.where(train_pred > 0.475, 1,0)\n",
        "  print(\"Training Accuracy: \", (np.abs(np.sum(train_pred == training_data_y))/len(training_data_y)))\n",
        "\n",
        "  test_pred = pred_eval(testing_data_X, W, bias)\n",
        "  test_pred = np.where(test_pred > 0.475, 1,0)\n",
        "  print(\"Testing Accuracy: \", (np.abs(np.sum(test_pred == testing_data_y))/len(testing_data_y)))\n",
        "  \n",
        "  validation_pred = pred_eval(validation_data_X, W, bias)\n",
        "  validation_pred = np.where(validation_pred > 0.475, 1,0)\n",
        "  print(\"Validation Accuracy: \", (np.abs(np.sum(validation_pred == validation_data_y))/len(validation_data_y)))"
      ],
      "metadata": {
        "id": "bFxvayZLJ2EH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resultQ1()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWFXAW0FJ3iA",
        "outputId": "4ebcdd81-781f-404a-ae89-e905eb09cf65"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Weights after training is as follows: \n",
            "\n",
            "array([ 0.85795909,  1.66906918, -2.09621634,  0.28076942,  0.30669305,\n",
            "       -0.97514536, -0.81478922, -0.10596437,  2.13440754,  0.84882177,\n",
            "        1.55784708,  1.07581791,  0.48570685, -1.95426305, -0.06741174,\n",
            "       -0.15370371,  0.56661061, -0.76339953,  0.81540497,  1.62162678,\n",
            "       -0.13283423, -0.57741426,  1.26541463,  1.04988975, -0.62619035,\n",
            "       -0.82056524,  0.57239927, -0.42799222,  1.42889965,  0.62555476,\n",
            "       -1.07298089,  0.02928202,  0.56806728, -0.23331988, -0.58169825,\n",
            "       -2.4625393 , -1.85496705,  0.00650336,  0.95276404, -0.36274683,\n",
            "        0.48488188,  0.38686099,  1.75117592, -0.04029829,  0.27997382,\n",
            "        0.0613423 ,  0.85331014,  0.74340836,  0.46711094, -1.88527751,\n",
            "        1.20728146,  0.84608659, -0.31657515,  2.02640599,  0.33511848,\n",
            "       -0.01587141,  0.41354977, -0.42870244,  0.19589594, -1.06725365])\n",
            "The Bias after training is as follows:  1.0000714293080224\n",
            "Training Accuracy:  0.9655172413793104\n",
            "Testing Accuracy:  0.7142857142857143\n",
            "Validation Accuracy:  0.6666666666666666\n"
          ]
        }
      ]
    }
  ]
}