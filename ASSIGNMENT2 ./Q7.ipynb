{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NNFL_A2_Q7.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKNHGofo-5Oy",
        "outputId": "08a70617-1d67-43f5-d1f7-a59e870b29c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing directory to the directory containing dataset\n",
        "%cd drive/MyDrive/NNFL data/Data_A2/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLV4lox_--8s",
        "outputId": "0e9636a8-64d2-4248-e57c-8430d9aef284"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/NNFL data/Data_A2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls -l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6T8nJVyy_NG9",
        "outputId": "2c63bf83-621b-4e46-ce09-9401fcebed85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 87\n",
            "-rw------- 1 root root   259 Apr 29 07:23 class_label.mat\n",
            "-rw------- 1 root root 70617 Apr 22 07:54 data55.xlsx\n",
            "-rw------- 1 root root 17039 Apr 29 07:25 data5.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "0EgDWGnb_SqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "aEUApvhb_UbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def metrics(Y_true, Y_pred):\n",
        "    FP=0 # For counting the False Positives\n",
        "    FN=0 # For counting the False Negatives\n",
        "    TN=0 # For counting the True Negatives\n",
        "    TP=0 # For counting the True Positives\n",
        "\n",
        "    for i in range(len(Y_true)):\n",
        "      if Y_true[i]==1:\n",
        "        if Y_pred[i]==1:\n",
        "          TP+=1\n",
        "        else:\n",
        "          FN+=1\n",
        "      else:\n",
        "        if Y_pred[i]==0:\n",
        "          TN+=1\n",
        "        else:\n",
        "          FP+=1\n",
        "\n",
        "    print('{}'.format('-'*75))\n",
        "    \n",
        "    sens= TP/(TP+FN)\n",
        "    spes = TN/(TN+FP)\n",
        "\n",
        "    print(\"Sensitivity : \", sens)\n",
        "    print(\"Specificity : \", spes)\n",
        "    print(\"Accuracy ((TN+TP)/(TN+TP+FN+FP)) : \", ((TP+TN)/(TN+FN+TP+FP)))"
      ],
      "metadata": {
        "id": "NG5wqfYiD2As"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoidFunction(Z):\n",
        "    return 1/(1+np.exp(-Z)),Z\n",
        "\n",
        "def relu(Z):\n",
        "    A = np.maximum(0,Z)\n",
        "    return Z,A\n",
        "\n",
        "def reluBackward(dA, cache):\n",
        "    Z = cache\n",
        "    \n",
        "    dZ = np.array(dA, copy=True) \n",
        "    dZ[Z <= 0] = 0\n",
        "    \n",
        "    return dZ\n",
        "\n",
        "def sigmoidBackward(dA, cache):\n",
        "    Z = cache\n",
        "    \n",
        "    s = 1/(1+np.exp(-Z))\n",
        "    dZ = dA * s * (1-s)\n",
        "\n",
        "    return dZ\n",
        "\n",
        "def initializeNewLayer(layer_dims,para,stack):\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)            \n",
        "    for l in range(1, L-1):\n",
        "        if stack==False:\n",
        "            parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1])\n",
        "        else:\n",
        "            parameters['W' + str(l)]=para[l-1]['W1']\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n",
        "    parameters['W' + str(L-1)] = np.random.randn(layer_dims[L-1],layer_dims[L-2])\n",
        "    parameters['b' + str(L-1)] = np.zeros((layer_dims[L-1],1))\n",
        "    return parameters\n",
        "\n",
        "def linearForward(A, W, b):\n",
        "    Z = np.dot(W,A)+b\n",
        "    cache = (A, W, b)\n",
        "    return Z,cache\n",
        "\n",
        "def linearActivationFunction(A_prev, W, b, activation):\n",
        "    \n",
        "    if(activation == \"sigmoid\"):\n",
        "        Z, linear_cache = linearForward(A_prev,W,b)\n",
        "        A, activation_cache = sigmoidFunction(Z)\n",
        "    \n",
        "    elif(activation == \"relu\"):\n",
        "        Z, linear_cache = linearForward(A_prev,W,b)\n",
        "        A, activation_cache = relu(Z)\n",
        "        \n",
        "    cache = (linear_cache, activation_cache)\n",
        "    \n",
        "    return A, cache\n",
        "\n",
        "def modelLForward(X, params):\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = (len(params) //2)\n",
        "    for l in range(1, L):\n",
        "        A_prev = A \n",
        "        A, cache = linearActivationFunction(A, params['W'+str(l)], params['b'+str(l)],\"sigmoid\")\n",
        "        caches.append(cache)\n",
        "    \n",
        "    AL, cache = linearActivationFunction(A, params['W'+str(L)], params['b'+str(L)],\"sigmoid\")\n",
        "    caches.append(cache)\n",
        "\n",
        "    return AL,caches\n",
        "\n",
        "def costComputation(AL,Y):\n",
        "    m = Y.shape[1]\n",
        "    cost = -(1/m)*np.sum(Y*np.log(AL)+(1-Y)*np.log(1-AL))\n",
        "    cost = np.squeeze(cost)\n",
        "    return cost\n",
        "\n",
        "def costComputationAE(AL,Y,parameters):\n",
        "    m=Y.shape[1]\n",
        "    cost=(1/(2*m))*np.sum(np.linalg.norm(AL-Y))\n",
        "    L = len(parameters) //2\n",
        "    return cost\n",
        "\n",
        "def linearBack(dZ, cache):\n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "    dW = (1/m)*np.dot(dZ,A_prev.T)\n",
        "    db = (1/m)*np.sum(dZ,axis=1,keepdims=True)\n",
        "    dA_prev = np.dot(W.T,dZ)\n",
        "   \n",
        "    return dA_prev, dW, db\n",
        "\n",
        "def linearActivation(dA, cache, activation):\n",
        "    linear_cache, activation_cache = cache\n",
        "    \n",
        "    if activation == \"relu\":\n",
        "        dZ = reluBackward(dA,activation_cache)\n",
        "        dA_prev, dW, db = linearBack(dZ,linear_cache)\n",
        "        \n",
        "    elif activation == \"sigmoid\":\n",
        "        dZ = sigmoidBackward(dA,activation_cache)\n",
        "        dA_prev, dW, db = linearBack(dZ,linear_cache)\n",
        "    \n",
        "    return dA_prev, dW, db\n",
        "\n",
        "def backModelL(AL, Y, caches):\n",
        "    grads = {}\n",
        "    L = len(caches) \n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape) \n",
        "    \n",
        "    dAL = -(np.divide(Y,AL)-np.divide(1-Y,1-AL))\n",
        "    \n",
        "    current_cache = caches[L-1]\n",
        "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linearActivation(dAL,current_cache,\"sigmoid\")\n",
        "   \n",
        "    for l in reversed(range(L-1)):\n",
        "    \n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = linearActivation(grads[\"dA\"+str(l+1)],current_cache,\"sigmoid\")\n",
        "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "        grads[\"db\" + str(l + 1)] = db_temp\n",
        "\n",
        "    return grads\n",
        "\n",
        "def updateParams(params, grads, learning_rate):\n",
        "   \n",
        "    L = len(params) //2\n",
        "\n",
        "    for l in range(L):\n",
        "        params[\"W\" + str(l+1)] = params[\"W\" + str(l+1)]-learning_rate*grads[\"dW\"+str(l+1)]\n",
        "        params[\"b\" + str(l+1)] = params[\"b\" + str(l+1)]-learning_rate*grads[\"db\"+str(l+1)]\n",
        "  \n",
        "    return params\n",
        "\n",
        "def layerLModel(X, Y, layers_dims, num_iterations,stack, learning_rate = 0.025):\n",
        "\n",
        "    costs = []                        \n",
        " \n",
        "    parameters = initializeNewLayer(layers_dims,para,stack)\n",
        "\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        AL, caches = modelLForward(X,parameters)\n",
        "        \n",
        "        if stack==True:\n",
        "            cost = costComputation(AL,Y)\n",
        "        else:\n",
        "            cost = costComputationAE(AL,Y,parameters)\n",
        "        \n",
        "        grads = backModelL(AL,Y,caches)\n",
        "     \n",
        "        parameters = updateParams(parameters,grads,learning_rate)            \n",
        "    \n",
        "    return (parameters, costs)\n",
        "\n",
        "para = []\n",
        "def ACT(x, a, b, act):\n",
        "    if(act == \"gaussian\"):\n",
        "        return np.exp(-b*np.linalg.norm(x-a))\n",
        "    elif(act == \"tanh\"):\n",
        "        num = 1 - np.exp(-(np.dot(x.T, a) + b))\n",
        "        den = 1 + np.exp(-(np.dot(x.T, a) + b))\n",
        "        return (num/den)\n",
        "\n",
        "def init(l_hidden, dimensions):\n",
        "    a = []\n",
        "    b = []\n",
        "    for i in range(l_hidden):\n",
        "        a.append(np.random.rand(dimensions,1))\n",
        "        b.append(np.random.rand(1))\n",
        "    return (a,b)\n",
        "\n",
        "def oneHotEnc(y):\n",
        "    from sklearn.preprocessing import OneHotEncoder \n",
        "    onehotencoder = OneHotEncoder() \n",
        "    y = onehotencoder.fit_transform(y).toarray()\n",
        "    return y\n",
        "\n",
        "def compute(l_hidden,training_data_X,testing_data_X,training_data_y,testing_data_y,act):\n",
        "\n",
        "    Y_enc = oneHotEnc(training_data_y)\n",
        "    H = np.zeros((training_data_X.shape[0],l_hidden))\n",
        "    for i in range(H.shape[1]):\n",
        "        for j in range(H.shape[0]):\n",
        "            H[j][i]=ACT(train_x[j],a[i],b[i],act)\n",
        "    W=np.dot(np.linalg.pinv(H),Y_enc)\n",
        "    \n",
        "    H=np.zeros((testing_data_X.shape[0],l_hidden))\n",
        "    for i in range(H.shape[1]):\n",
        "        for j in range(H.shape[0]):\n",
        "            H[j][i]=ACT(testing_data_X[j],a[i],b[i],act)\n",
        "\n",
        "    p = np.dot(H,W)\n",
        "    p = np.argmax(p,axis=1)\n",
        "    p = np.reshape(p,newshape=(p.shape[0],1))\n",
        "    metrics(p, testing_data_y)\n",
        "\n",
        "\n",
        "dataset = pd.read_excel('/content/drive/MyDrive/NNFL data/Data_A2/data55.xlsx')\n",
        "\n",
        "row, col = dataset.shape\n",
        "feats = col - 1 \n",
        "\n",
        "# normalization\n",
        "dataset.loc[:, dataset.columns != feats] = (dataset.loc[:, dataset.columns != feats]-dataset.loc[:, dataset.columns != feats].mean(axis=0))/dataset.loc[:, dataset.columns != feats].std(axis=0)\n",
        "\n",
        "# spliting dataset into train test and val\n",
        "training_data, validation_data, testing_data = np.split(dataset.sample(frac=1),[int(0.7*len(dataset)), int(0.8*len(dataset))])\n",
        "\n",
        "training_data = np.array(training_data)\n",
        "validation_data = np.array(validation_data)\n",
        "testing_data = np.array(testing_data)\n",
        "training_data_X = training_data[:, :feats]\n",
        "training_data_y = training_data[:, feats]\n",
        "validation_data_X = validation_data[:, :feats]\n",
        "validation_data_y = validation_data[:, feats]\n",
        "testing_data_X = testing_data[:, :feats]\n",
        "testing_data_y = testing_data[:, feats]\n",
        "\n",
        "train_row, train_col = training_data_X.shape\n",
        "\n",
        "layers_dims = [72,32,72]\n",
        "parameters, costs = layerLModel(training_data_X, training_data_y, layers_dims, num_iterations=1500, stack=False, learning_rate=0.005)\n",
        "paramsAE1 = parameters\n",
        "\n",
        "W1 = paramsAE1['W1']\n",
        "b1 = paramsAE1['b1']\n",
        "X_new, _ = linearActivationFunction(X.T,W1,b1,\"sigmoid\")\n",
        "\n",
        "layers_dims = [32,16,32]\n",
        "parameters,costs = layerLModel(training_data_X, training_data_y, layers_dims,num_iterations=1500,stack=False,learning_rate=0.05, print_cost = True)\n",
        "paramsAE2 = parameters\n",
        "\n",
        "W1 = paramsAE1['W1']\n",
        "W2 = paramsAE2['W1']\n",
        "b1 = paramsAE1['b1']\n",
        "b2 = paramsAE2['b1']\n",
        "\n",
        "x, _ = sigmoidFunction(np.dot(W1,X.T) + b1)\n",
        "x, _ = sigmoidFunction(np.dot(W2,x) + b2)\n",
        "\n",
        "a, b = init(256, X.shape[1])\n",
        "print(\"Tanh Accuracy: \")\n",
        "compute(256, training_data_X, testing_data_X, training_data_y, testing_data_y, \"tanh\")\n",
        "print()\n",
        "a, b = init(256,X.shape[1])\n",
        "print(\"Gaussian Accuracy: \")\n",
        "compute(256,training_data_X,testing_data_X,training_data_y,testing_data_y,\"gaussian\")"
      ],
      "metadata": {
        "id": "_lXSZJQdCxhO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}